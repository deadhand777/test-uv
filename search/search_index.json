{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"test-uv","text":"<p>This is a test of the new project templete for ai show cases.</p> <ul> <li>Github repository: https://github.com/deadhand777/test-uv/</li> <li>Documentation https://deadhand777.github.io/test-uv/</li> </ul>"},{"location":"#getting-started-with-your-project","title":"Getting started with your project","text":""},{"location":"#1-create-a-new-repository","title":"1. Create a New Repository","text":"<p>First, create a repository on GitHub with the same name as this project, and then run the following commands:</p> <pre><code>git init -b main\ngit add .\ngit commit -m \"init commit\"\ngit remote add origin git@github.com:deadhand777/test-uv.git\ngit push -u origin main\n</code></pre>"},{"location":"#2-set-up-your-development-environment","title":"2. Set Up Your Development Environment","text":"<p>Then, install the environment and the pre-commit hooks with</p> <pre><code>make install\n</code></pre> <p>This will also generate your <code>uv.lock</code> file</p>"},{"location":"#3-run-the-pre-commit-hooks","title":"3. Run the pre-commit hooks","text":"<p>Initially, the CI/CD pipeline might be failing due to formatting issues. To resolve those run:</p> <pre><code>uv run pre-commit run -a\n</code></pre>"},{"location":"#4-commit-the-changes","title":"4. Commit the changes","text":"<p>Lastly, commit the changes made by the two steps above to your repository.</p> <pre><code>git add .\ngit commit -m 'Fix formatting issues'\ngit push origin main\n</code></pre> <p>You are now ready to start development on your project! The CI/CD pipeline will be triggered when you open a pull request, merge to main, or when you create a new release.</p> <p>To finalize the set-up for publishing to PyPI, see here. For activating the automatic documentation with MkDocs, see here. To enable the code coverage reports, see here.</p>"},{"location":"#releasing-a-new-version","title":"Releasing a new version","text":"<p>Repository initiated with fpgmaas/cookiecutter-uv.</p>"},{"location":"#uv","title":"UV","text":"<pre><code>uv venv --python 3.11.6\nsource .venv/bin/activate\nwhich python\n python -V\nuv sync\n uv pip list\nuv add &lt;package&gt;\nuv lock\n</code></pre>"},{"location":"#linting-styling","title":"Linting &amp; Styling","text":"<pre><code> test-section\n</code></pre>"},{"location":"#documentation-from-mkdocs","title":"Documentation from MKDocs","text":"<pre><code>make docs\n</code></pre>"},{"location":"modules/","title":"Modules","text":""},{"location":"modules/#utility-modules","title":"Utility modules","text":"<p>This section includes common utility functions and classes.</p>"},{"location":"modules/#src.code.utils.check_bool_type","title":"<code>check_bool_type(boolean)</code>","text":"<p>Checks that the given argument is a boolean.</p> <p>Parameters:</p> Name Type Description Default <code>boolean</code> <code>bool</code> <p>The boolean to check.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If boolean is not a boolean.</p> Source code in <code>src/code/utils.py</code> <pre><code>def check_bool_type(boolean: bool) -&gt; None:\n    \"\"\"\n    Checks that the given argument is a boolean.\n\n    Args:\n        boolean (bool): The boolean to check.\n\n    Raises:\n        TypeError: If boolean is not a boolean.\n    \"\"\"\n    if not isinstance(boolean, bool):\n        raise TypeError(\"script_mode must be a boolean\")\n</code></pre>"},{"location":"modules/#src.code.utils.check_s3_path","title":"<code>check_s3_path(s3_path)</code>","text":"<p>Checks that the given string is a valid S3 path.</p> <p>Parameters:</p> Name Type Description Default <code>s3_path</code> <code>str</code> <p>The S3 path to check.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If s3_path is not a string.</p> <code>ValueError</code> <p>If s3_path does not start with 's3://'.</p> Source code in <code>src/code/utils.py</code> <pre><code>def check_s3_path(s3_path: str | None) -&gt; None:\n    \"\"\"\n    Checks that the given string is a valid S3 path.\n\n    Args:\n        s3_path (str): The S3 path to check.\n\n    Raises:\n        TypeError: If s3_path is not a string.\n        ValueError: If s3_path does not start with 's3://'.\n    \"\"\"\n    if not isinstance(s3_path, str):\n        raise TypeError(\"model_path must be a string\")\n\n    if not s3_path.startswith(\"s3://\"):\n        raise ValueError(f\"{s3_path} must start with 's3://'\")\n</code></pre>"},{"location":"modules/#src.code.utils.check_string_type","title":"<code>check_string_type(string)</code>","text":"<p>Checks that the given argument is a string.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>The string to check.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If string is not a string.</p> Source code in <code>src/code/utils.py</code> <pre><code>def check_string_type(string: str | None) -&gt; None:\n    \"\"\"\n    Checks that the given argument is a string.\n\n    Args:\n        string (str): The string to check.\n\n    Raises:\n        TypeError: If string is not a string.\n    \"\"\"\n    if not isinstance(string, str):\n        raise TypeError(f\"Parameter {string} must be a string\")\n</code></pre>"},{"location":"modules/#src.code.utils.compress_joblib_model","title":"<code>compress_joblib_model(model_path, output_path=None, verbose=True)</code>","text":"<p>Compresses a scikit-learn model saved as a .joblib file into a .tar.gz archive.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>The path to the saved .joblib file.</p> required <code>output_path</code> <code>str | None</code> <p>The path for the output .tar.gz file. If not provided, it will use the same directory as the model file.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, print the S3 path to which the data was uploaded.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the compressed .tar.gz file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; compress_joblib_model(model_path=\"model.joblib\")\n</code></pre> Source code in <code>src/code/utils.py</code> <pre><code>def compress_joblib_model(model_path: str, output_path: str | None = None, verbose: bool = True) -&gt; str:\n    \"\"\"\n    Compresses a scikit-learn model saved as a .joblib file into a .tar.gz archive.\n\n    Args:\n        model_path (str): The path to the saved .joblib file.\n        output_path (str | None, optional): The path for the output .tar.gz file. If not provided, it will use the same directory as the model file.\n        verbose (bool): If True, print the S3 path to which the data was uploaded.\n\n    Returns:\n        str: The path to the compressed .tar.gz file.\n\n    Examples:\n        &gt;&gt;&gt; compress_joblib_model(model_path=\"model.joblib\")\n    \"\"\"\n\n    # Run checks\n    check_string_type(model_path)\n\n    assert os.path.exists(model_path)\n    assert model_path.endswith(\".joblib\")\n\n    # Set the default output path if not provided\n    if output_path is None:\n        output_path = f\"{model_path.removesuffix('.joblib')}.tar.gz\"\n\n    # Create a tar.gz archive and add the joblib model file to it\n    with tarfile.open(output_path, \"w:gz\") as tar:\n        tar.add(model_path, arcname=os.path.basename(model_path))\n\n    if verbose:\n        print(f\"Model compressed as .tar.gz archive and saved to: {output_path}\")\n\n    return output_path\n</code></pre>"},{"location":"modules/#src.code.utils.delete_s3_objects","title":"<code>delete_s3_objects(s3_data_path)</code>","text":"<p>Deletes all objects in the specified S3 bucket with the given key prefix.</p> <p>Parameters:</p> Name Type Description Default <code>s3_data_path</code> <code>str</code> <p>The path to the S3 bucket and key prefix.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; s3_data_path = \"s3://sagemaker-eu-central-1-763678331342/foo\"\n&gt;&gt;&gt; delete_s3_objects(s3_data_path=s3_data_path)\n</code></pre> Source code in <code>src/code/utils.py</code> <pre><code>def delete_s3_objects(\n    s3_data_path: str,\n) -&gt; None:\n    \"\"\"\n    Deletes all objects in the specified S3 bucket with the given key prefix.\n\n    Args:\n        s3_data_path (str): The path to the S3 bucket and key prefix.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; s3_data_path = \"s3://sagemaker-eu-central-1-763678331342/foo\"\n        &gt;&gt;&gt; delete_s3_objects(s3_data_path=s3_data_path)\n    \"\"\"\n\n    # Run checks\n    check_s3_path(s3_data_path)\n\n    # Delete data from S3\n    bucket, key_prefix = s3_data_path[5:].split(\"/\", 1)\n    s3_client: boto3.client = boto3.client(\"s3\")\n\n    response = s3_client.list_objects_v2(Bucket=bucket, Prefix=key_prefix)\n\n    if \"Contents\" in response:\n        objects = [response.get(\"Contents\")[i].get(\"Key\") for i in range(len(response.get(\"Contents\")))]\n        logger.info(f\"Objects in bucket: {objects}\")\n        [s3_client.delete_object(Bucket=bucket, Key=obj[\"Key\"]) for obj in response.get(\"Contents\")]\n        logger.info(f\"Deleted objects in bucket: {bucket}/{key_prefix}\")\n    else:\n        logger.info(f\"No objects in bucket: {bucket}/{key_prefix}\")\n</code></pre>"},{"location":"modules/#src.code.utils.delete_s3_prefix","title":"<code>delete_s3_prefix(s3_data_path)</code>","text":"<p>Deletes all objects in the specified S3 bucket with the given key prefix.</p> <p>Parameters:</p> Name Type Description Default <code>s3_data_path</code> <code>str</code> <p>The path to the S3 bucket and key prefix.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; s3_path: str = \"s3://sagemaker-eu-central-1-763678331342/foo\"\n&gt;&gt;&gt; delete_s3_prefix(s3_data_path=s3_path)\n</code></pre> Source code in <code>src/code/utils.py</code> <pre><code>def delete_s3_prefix(s3_data_path: str) -&gt; None:\n    \"\"\"\n    Deletes all objects in the specified S3 bucket with the given key prefix.\n\n    Args:\n        s3_data_path (str): The path to the S3 bucket and key prefix.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; s3_path: str = \"s3://sagemaker-eu-central-1-763678331342/foo\"\n        &gt;&gt;&gt; delete_s3_prefix(s3_data_path=s3_path)\n    \"\"\"\n\n    # Run checks\n    check_s3_path(s3_data_path)\n\n    # Delete data from S3\n    bucket, key_prefix = s3_data_path[5:].split(\"/\", 1)\n    s3_client: boto3.client = boto3.resource(\"s3\")\n    s3_client.Bucket(bucket).objects.filter(Prefix=key_prefix).delete()\n    logger.info(f\"Deleted bucket prefix: {bucket}/{key_prefix}\")\n</code></pre>"},{"location":"modules/#src.code.utils.pd_read_s3_parquet","title":"<code>pd_read_s3_parquet(s3_data_path, **args)</code>","text":"<p>Reads a Parquet file from an S3 bucket and returns a Pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>s3_data_path</code> <code>str</code> <p>The S3 location of the Parquet file.</p> required <code>**args</code> <code>Any</code> <p>The keyword arguments to pass to pd.read_parquet.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas.DataFrame: The Parquet data as a Pandas DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; s3_path: str = \"s3://sagemaker-eu-central-1-763678331342/scikit-learn/demo/data/data.parquet\"\n&gt;&gt;&gt; df: pd.DataFrame = pd_read_s3_parquet(s3_data_path=s3_path)\n</code></pre> Source code in <code>src/code/utils.py</code> <pre><code>def pd_read_s3_parquet(\n    s3_data_path: str,\n    **args: Any,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads a Parquet file from an S3 bucket and returns a Pandas DataFrame.\n\n    Args:\n        s3_data_path (str): The S3 location of the Parquet file.\n        **args: The keyword arguments to pass to pd.read_parquet.\n\n    Returns:\n        pandas.DataFrame: The Parquet data as a Pandas DataFrame.\n\n    Examples:\n        &gt;&gt;&gt; s3_path: str = \"s3://sagemaker-eu-central-1-763678331342/scikit-learn/demo/data/data.parquet\"\n        &gt;&gt;&gt; df: pd.DataFrame = pd_read_s3_parquet(s3_data_path=s3_path)\n    \"\"\"\n\n    # Run checks\n    check_s3_path(s3_data_path)\n\n    # Get data from S3\n    bucket, key_prefix = s3_data_path[5:].split(\"/\", 1)\n    s3_client: boto3.client = boto3.client(\"s3\")\n    obj: dict = s3_client.get_object(Bucket=bucket, Key=key_prefix)\n\n    # Convert to Pandas DataFrame\n    df: pd.DataFrame = pd.read_parquet(io.BytesIO(obj[\"Body\"].read()), **args)\n\n    return df\n</code></pre>"},{"location":"modules/#src.code.utils.upload_data","title":"<code>upload_data(local_data_path, s3_data_path, verbose=True)</code>","text":"<p>Uploads a local data file to an S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>local_data_path</code> <code>str</code> <p>The path to the local data file.</p> required <code>s3_data_path</code> <code>str</code> <p>The path in the S3 bucket to which the data file should be uploaded.</p> required <code>verbose</code> <code>bool</code> <p>If True, print the S3 path to which the data was uploaded.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; local_path: str = \"data/data.parquet\"\n&gt;&gt;&gt; s3_path: str = \"s3://sagemaker-eu-central-1-763678331342/scikit-learn/demo/data\"\n&gt;&gt;&gt; upload_data(local_data_path = local_path, s3_data_path = s3_path)\n</code></pre> Source code in <code>src/code/utils.py</code> <pre><code>def upload_data(local_data_path: str, s3_data_path: str, verbose: bool = True) -&gt; None:\n    \"\"\"\n    Uploads a local data file to an S3 bucket.\n\n    Args:\n        local_data_path (str): The path to the local data file.\n        s3_data_path (str): The path in the S3 bucket to which the data file should be uploaded.\n        verbose (bool): If True, print the S3 path to which the data was uploaded.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; local_path: str = \"data/data.parquet\"\n        &gt;&gt;&gt; s3_path: str = \"s3://sagemaker-eu-central-1-763678331342/scikit-learn/demo/data\"\n        &gt;&gt;&gt; upload_data(local_data_path = local_path, s3_data_path = s3_path)\n    \"\"\"\n\n    # Run checks\n    check_string_type(local_data_path)\n    check_s3_path(s3_data_path)\n\n    # Upload data to S3\n    bucket, key_prefix = s3_data_path[5:].split(\"/\", 1)\n    session: sagemaker.Session = sagemaker.Session()\n    session.upload_data(local_data_path, bucket=bucket, key_prefix=key_prefix)\n\n    if verbose:\n        print(f\"Data uploaded to {s3_data_path}/{local_data_path.split('/')[-1]}\")\n</code></pre>"},{"location":"modules/#src.code.utils.upload_model","title":"<code>upload_model(local_model_path, s3_model_path, verbose=True)</code>","text":"<p>Uploads a local model file to an S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>local_model_path</code> <code>str</code> <p>The path to the local model file.</p> required <code>s3_model_path</code> <code>str</code> <p>The path to the S3 bucket where the model should be uploaded.</p> required <code>verbose</code> <code>bool</code> <p>If True, print the S3 path to which the data was uploaded.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; local_path: str = \"./script_mode/models/model.tar.gz\"\n&gt;&gt;&gt; s3_path: str = \"s3://sagemaker-eu-central-1-763678331342/scikit-learn/demo/models\"\n&gt;&gt;&gt; upload_model(local_model_path=local_path, s3_model_path=s3_path)\n</code></pre> Source code in <code>src/code/utils.py</code> <pre><code>def upload_model(local_model_path: str, s3_model_path: str, verbose: bool = True) -&gt; None:\n    \"\"\"\n    Uploads a local model file to an S3 bucket.\n\n    Args:\n        local_model_path (str): The path to the local model file.\n        s3_model_path (str): The path to the S3 bucket where the model should be uploaded.\n        verbose (bool): If True, print the S3 path to which the data was uploaded.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; local_path: str = \"./script_mode/models/model.tar.gz\"\n        &gt;&gt;&gt; s3_path: str = \"s3://sagemaker-eu-central-1-763678331342/scikit-learn/demo/models\"\n        &gt;&gt;&gt; upload_model(local_model_path=local_path, s3_model_path=s3_path)\n    \"\"\"\n\n    # Run checks\n    check_string_type(local_model_path)\n    check_s3_path(s3_model_path)\n\n    # Upload model to S3\n    bucket, key_prefix = s3_model_path[5:].split(\"/\", 1)\n    session: sagemaker.Session = sagemaker.Session()\n    session.upload_data(local_model_path, bucket=bucket, key_prefix=key_prefix)\n\n    if verbose:\n        print(f\"Model uploaded to {s3_model_path}/{local_model_path.split('/')[-1]}\")\n</code></pre>"},{"location":"modules/#training-module","title":"Training module","text":"<p>This module includes functions and classes used for model training and inference.</p>"},{"location":"modules/#src.code.train.input_fn","title":"<code>input_fn(request_body, content_type)</code>","text":"<p>Deserialize input data into a format that can be used by the model.</p> <p>Parameters:</p> Name Type Description Default <code>request_body</code> <code>str</code> <p>The body of the request sent to the model.</p> required <code>content_type</code> <code>str</code> <p>The content type of the request.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The input data in a format that can be used by the model.</p> Source code in <code>src/code/train.py</code> <pre><code>def input_fn(request_body: str, content_type: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Deserialize input data into a format that can be used by the model.\n\n    Parameters:\n        request_body (str): The body of the request sent to the model.\n        content_type (str): The content type of the request.\n\n    Returns:\n        pd.DataFrame: The input data in a format that can be used by the model.\n    \"\"\"\n    expected_features: list[str] = [\"sepal_len\", \"sepal_wid\", \"petal_len\", \"petal_wid\"]\n\n    if content_type == \"application/json\":\n        # Extract features from JSON key-value pairs\n        data: dict[str, float] = json.loads(request_body)\n\n        # Data validation and processing\n        try:\n            data_valid: Data = Data(**data)\n            logger.info(f\"Data is valid: {data_valid.model_dump_json()}\")\n\n            complete_data: dict[str, float] = {feature: data_valid.model_dump().get(feature, np.nan) for feature in expected_features}\n            df: pd.DataFrame = pd.DataFrame.from_dict(complete_data, orient=\"index\").T\n            return df\n        except ValidationError as e:\n            logger.error(e.errors())\n    else:\n        raise ValueError(f\"Unsupported content type: {content_type}\")\n</code></pre>"},{"location":"modules/#src.code.train.load_data","title":"<code>load_data(data_path)</code>","text":"<p>Loads data from a given path and splits it into X and y.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>The path to the data file.</p> required <p>Returns:</p> Name Type Description <code>X</code> <code>DataFrame</code> <p>The features.</p> <code>y</code> <code>Series</code> <p>The target.</p> Source code in <code>src/code/train.py</code> <pre><code>def load_data(data_path: str) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Loads data from a given path and splits it into X and y.\n\n    Args:\n        data_path (str): The path to the data file.\n\n    Returns:\n        X (pd.DataFrame): The features.\n        y (pd.Series): The target.\n    \"\"\"\n    if not isinstance(data_path, str):\n        raise TypeError(\"local_model_path must be a string\")\n\n    df: pd.DataFrame = pd.read_parquet(data_path, engine=\"pyarrow\")\n    X: pd.DataFrame = df.drop(\"class\", axis=1)\n    y: pd.Series = df[\"class\"]\n\n    return X, y\n</code></pre>"},{"location":"modules/#src.code.train.model_fn","title":"<code>model_fn(model_path)</code>","text":"<p>Deserialize fitted model. Loads a saved model from a file in the model directory.</p> <p>Parameters: model_path (str): The path to the directory containing the saved model.</p> <p>Returns: model (object): The loaded model object.</p> Source code in <code>src/code/train.py</code> <pre><code>def model_fn(model_path: str) -&gt; object:\n    \"\"\"\n    Deserialize fitted model. Loads a saved model from a file in the model directory.\n\n    Parameters:\n    model_path (str): The path to the directory containing the saved model.\n\n    Returns:\n    model (object): The loaded model object.\n    \"\"\"\n    model = joblib.load(os.path.join(model_path, \"model.joblib\"))\n    return model\n</code></pre>"},{"location":"modules/#src.code.train.predict_fn","title":"<code>predict_fn(input_data, model)</code>","text":"<p>Make predictions against input_data using the model provided.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>ndarray</code> <p>The input data to make predictions against.</p> required <code>model</code> <code>sklearn model</code> <p>The model to use for prediction.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Union[int, ndarray]]</code> <p>A dictionary with two keys, 'predicted_class' and 'predicted_probabilities'.     'predicted_class' is the predicted class, and 'predicted_probabilities' is the     probability distribution of all classes.</p> Source code in <code>src/code/train.py</code> <pre><code>def predict_fn(input_data: Union[np.ndarray, pd.DataFrame], model: sklearn.pipeline.Pipeline) -&gt; dict[str, Union[int, np.ndarray]]:\n    \"\"\"\n    Make predictions against input_data using the model provided.\n\n    Args:\n        input_data (numpy.ndarray): The input data to make predictions against.\n        model (sklearn model): The model to use for prediction.\n\n    Returns:\n        dict: A dictionary with two keys, 'predicted_class' and 'predicted_probabilities'.\n                'predicted_class' is the predicted class, and 'predicted_probabilities' is the\n                probability distribution of all classes.\n    \"\"\"\n\n    # Collect predictions\n    predicted_class: int = model.predict(input_data)[0]\n    predicted_proba: np.ndarray = model.predict_proba(input_data)[0]\n\n    # Return predictions\n    result: dict[str, Union[int, np.ndarray]] = dict(\n        predicted_class=predicted_class,\n        predicted_probabilities=predicted_proba.tolist(),\n    )\n    return result\n</code></pre>"},{"location":"modules/#deployment-module","title":"Deployment module","text":"<p>This module includes functions and classes used for model deployment.</p>"},{"location":"modules/#src.script.deploy.deploy_sklearn_model","title":"<code>deploy_sklearn_model(name, role, image_uri, script_mode=False, training_input_path=None, model_data=None, entry_point='train.py', source_dir='./src/code', hyperparameters=None, framework_version='1.2-1', py_version='py3')</code>","text":"<p>Deploy a Scikit-learn model using SageMaker. This function handles both training a new model in script mode and deploying an existing model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The base name for the model and endpoint.</p> required <code>role</code> <code>str</code> <p>The AWS IAM role for SageMaker permissions.</p> required <code>image_uri</code> <code>str</code> <p>The URI of the Docker image to use for training/deployment.</p> required <code>script_mode</code> <code>bool</code> <p>Whether to use script mode for training a new model.</p> <code>False</code> <code>training_input_path</code> <code>Optional[str]</code> <p>S3 path to training data for script mode.</p> <code>None</code> <code>model_data</code> <code>Optional[str]</code> <p>S3 path to the model artifact for existing model mode.</p> <code>None</code> <code>entry_point</code> <code>str</code> <p>The Python script for training or inference.</p> <code>'train.py'</code> <code>source_dir</code> <code>str</code> <p>The directory containing the entry point script.</p> <code>'./src/code'</code> <code>hyperparameters</code> <code>Optional[dict]</code> <p>Hyperparameters for the training job.</p> <code>None</code> <code>framework_version</code> <code>str</code> <p>The version of Scikit-learn to use.</p> <code>'1.2-1'</code> <code>py_version</code> <code>str</code> <p>The Python version to use.</p> <code>'py3'</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[SKLearn, SKLearnPredictor, dict[str, str], dict[str, str]]</code> <p>A tuple containing the model, predictor, model description, and endpoint description.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; MODEL_PREFIX: str = \"scikit-learn-script-mode-inference\"\n&gt;&gt;&gt; role: str = \"arn:aws:iam::763678331342:role/temp_sagemaker_role\"\n&gt;&gt;&gt; MODEL_DATA: str = \"s3://sagemaker-eu-central-1-763678331342/demo\"\n&gt;&gt;&gt; model, predictor, model_desc, endpoint_desc = deploy_sklearn_model(\n&gt;&gt;&gt;     name=MODEL_PREFIX,\n&gt;&gt;&gt;     model_data=model_data,\n&gt;&gt;&gt;     role=role\n&gt;&gt;&gt; )\n</code></pre> Source code in <code>src/script/deploy.py</code> <pre><code>def deploy_sklearn_model(\n    name: str,\n    role: str,\n    image_uri: str,\n    script_mode: bool = False,\n    training_input_path: Optional[str] = None,\n    model_data: Optional[str] = None,\n    entry_point: str = \"train.py\",\n    source_dir: str = \"./src/code\",\n    hyperparameters: Optional[dict[str, int | str | float | bool | None]] = None,\n    framework_version: str = \"1.2-1\",\n    py_version: str = \"py3\",\n) -&gt; tuple[\n    sagemaker.sklearn.SKLearn,\n    sagemaker.sklearn.model.SKLearnPredictor,\n    dict[str, str],\n    dict[str, str],\n]:\n    \"\"\"\n    Deploy a Scikit-learn model using SageMaker. This function handles both training\n    a new model in script mode and deploying an existing model.\n\n    Args:\n        name (str): The base name for the model and endpoint.\n        role (str): The AWS IAM role for SageMaker permissions.\n        image_uri (str): The URI of the Docker image to use for training/deployment.\n        script_mode (bool): Whether to use script mode for training a new model.\n        training_input_path (Optional[str]): S3 path to training data for script mode.\n        model_data (Optional[str]): S3 path to the model artifact for existing model mode.\n        entry_point (str): The Python script for training or inference.\n        source_dir (str): The directory containing the entry point script.\n        hyperparameters (Optional[dict]): Hyperparameters for the training job.\n        framework_version (str): The version of Scikit-learn to use.\n        py_version (str): The Python version to use.\n\n    Returns:\n        tuple: A tuple containing the model, predictor, model description, and endpoint description.\n\n    Examples:\n        &gt;&gt;&gt; MODEL_PREFIX: str = \"scikit-learn-script-mode-inference\"\n        &gt;&gt;&gt; role: str = \"arn:aws:iam::763678331342:role/temp_sagemaker_role\"\n        &gt;&gt;&gt; MODEL_DATA: str = \"s3://sagemaker-eu-central-1-763678331342/demo\"\n        &gt;&gt;&gt; model, predictor, model_desc, endpoint_desc = deploy_sklearn_model(\n        &gt;&gt;&gt;     name=MODEL_PREFIX,\n        &gt;&gt;&gt;     model_data=model_data,\n        &gt;&gt;&gt;     role=role\n        &gt;&gt;&gt; )\n    \"\"\"\n    # Set up logging\n    logger.add(\n        sink=\"train.log\",\n        format=\"{time:YYYY-MM-DD HH:MM:SS} | {level} | {message}\",\n        rotation=\"10 MB\",\n        retention=\"1 week\",\n        compression=\"zip\",\n    )\n\n    # Input validation\n    check_string_type(name)\n    check_string_type(role)\n    check_string_type(image_uri)\n    check_string_type(entry_point)\n    check_string_type(source_dir)\n    check_string_type(framework_version)\n    check_string_type(py_version)\n    check_bool_type(script_mode)\n\n    # Set model name with a timestamp\n    model_name: str = f\"{name}-{datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n\n    if script_mode:\n        logger.info(\"Script mode deployment initiated...\")\n\n        check_string_type(training_input_path)\n        check_s3_path(training_input_path)\n\n        # Create the SageMaker SKLearn model for training\n        logger.info(\"Training SageMaker SKLearn model...\")\n        model: sagemaker.sklearn.SKLearn = SKLearn(\n            base_job_name=model_name,\n            name=model_name,\n            entry_point=entry_point,\n            role=role,\n            instance_count=1,\n            instance_type=\"ml.c5.xlarge\",\n            py_version=py_version,\n            framework_version=framework_version,\n            image_uri=image_uri,\n            script_mode=True,\n            source_dir=source_dir,\n            hyperparameters=hyperparameters,\n        )\n\n        # Train the estimator\n        logger.info(\"Training the estimator...\")\n        model.fit({\"train\": training_input_path})\n\n    else:\n        logger.info(\"Existing model deployment initiated...\")\n\n        check_s3_path(model_data)\n\n        # Create the SageMaker SKLearn model for deployment\n        logger.info(\"Creating SageMaker SKLearn model...\")\n        model: sagemaker.sklearn.model.SKLearnModel = SKLearnModel(\n            name=model_name,\n            model_data=model_data,\n            role=role,\n            entry_point=entry_point,\n            py_version=py_version,\n            framework_version=framework_version,\n            image_uri=image_uri,\n            source_dir=source_dir,\n        )\n\n    # Prepare model description\n    model_desc: dict[str, str] = dict(\n        model_name=model.name if hasattr(model, \"name\") else None,\n        base_job_name=model.base_job_name if hasattr(model, \"base_job_name\") else None,\n        current_job_name=(model._current_job_name if hasattr(model, \"_current_job_name\") else None),\n        model_data=model.model_data,\n        image_uri=model.image_uri,\n        source_dir=model.source_dir,\n        entry_point=model.entry_point,\n        role=model.role,\n        framework_version=model.framework_version,\n        py_version=model.py_version,\n        model_dependencies=model.dependencies,\n    )\n\n    # Create SageMaker Endpoint\n    logger.info(\"Creating SageMaker Endpoint...\")\n    endpoint_name: str = f\"{name}-{datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n\n    # Deploy the Model as a SageMaker Endpoint\n    predictor: sagemaker.sklearn.model.SKLearnPredictor = model.deploy(\n        initial_instance_count=1,\n        instance_type=\"ml.c5.large\",\n        endpoint_name=endpoint_name,\n        serializer=JSONSerializer(),\n    )\n    logger.info(\"Model deployed successfully!\")\n\n    # Describe the SageMaker Endpoint\n    endpoint_context: sagemaker.lineage.context.EndpointContext = predictor.endpoint_context()\n    RUN_TIME_URL_PREFIX: str = \"https://runtime.sagemaker.eu-central-1.amazonaws.com/endpoints/\"\n    RUN_TIME_URL_SUFFIX: str = \"/invocations\"\n    logger.info(f\"Model runtime URL: {RUN_TIME_URL_PREFIX}{predictor.endpoint_name}{RUN_TIME_URL_SUFFIX}\")\n\n    # Prepare endpoint description\n    endpoint_desc: dict[str, str] = dict(\n        endpoint_name=predictor.endpoint_name,\n        endpoint_creation_time=endpoint_context.creation_time.strftime(\"%Y-%m-%d-%H-%M-%S\"),\n        endpoint_context_name=endpoint_context.context_name,\n        run_time_url=f\"{RUN_TIME_URL_PREFIX}{predictor.endpoint_name}{RUN_TIME_URL_SUFFIX}\",\n    )\n\n    return model, predictor, model_desc, endpoint_desc\n</code></pre>"},{"location":"setup/","title":"Setup","text":""},{"location":"setup/#getting-started-with-your-project","title":"Getting started with your project","text":""},{"location":"setup/#1-create-a-new-repository-with-this-template","title":"1. Create a New Repository with this template","text":"<p>First, create a repository on GitHub with the same name as this project, and then run the following commands:</p> <pre><code>git init -b main\ngit add .\ngit commit -m \"init commit\"\ngit remote add origin git@github.com:deadhand777/test-uv.git\ngit push -u origin main\n</code></pre>"},{"location":"setup/#2-set-up-your-development-environment","title":"2. Set Up Your Development Environment","text":"<p>Then, install the environment and the pre-commit hooks with</p> <pre><code>make install\n</code></pre> <p>This will also generate your <code>uv.lock</code> file</p>"},{"location":"setup/#3-run-the-pre-commit-hooks","title":"3. Run the pre-commit hooks","text":"<p>Initially, the CI/CD pipeline might be failing due to formatting issues. To resolve those run:</p> <pre><code>uv run pre-commit run -a\n</code></pre>"},{"location":"setup/#4-commit-the-changes","title":"4. Commit the changes","text":"<p>Lastly, commit the changes made by the two steps above to your repository.</p> <pre><code>git add .\ngit commit -m 'Fix formatting issues'\ngit push origin main\n</code></pre> <p>You are now ready to start development on your project! The CI/CD pipeline will be triggered when you open a pull request, merge to main, or when you create a new release.</p>"},{"location":"setup/#set-up-your-development-environment","title":"Set Up Your Development Environment","text":"<pre><code>uv venv --python &lt;python-version&gt; # e.g. 3.11.6\nsource .venv/bin/activate\nwhich python\npython -V\n</code></pre> <pre><code>uv sync\nuv pip list\nuv add &lt;package&gt;\nuv lock\n</code></pre>"},{"location":"setup/#update-projecttoml","title":"Update project.toml","text":"<ul> <li>add the following to <code>project.toml</code> if additional folders are present in the root directory:</li> </ul> <pre><code>[tool.setuptools]\npy-modules = [\"test_uv\", \"cdk\", \"additional_modules\"]\n</code></pre> <ul> <li>modify the following to <code>project.toml</code> to avoid deprecation warnings:</li> </ul> <pre><code>[tool.ruff]\n\n'ignore' -&gt; 'lint.ignore'\n'select' -&gt; 'lint.select'\n\n'per-file-ignores' -&gt; 'lint.per-file-ignores' in [tool.ruff.lint.per-file-ignores]\n</code></pre>"},{"location":"setup/#linting-styling","title":"Linting &amp; Styling","text":"<ul> <li>Run ruff:</li> </ul> <pre><code>uv run ruff format\n</code></pre> <ul> <li>Run pre-commit hooks:</li> </ul> <pre><code>make check\n</code></pre>"},{"location":"setup/#testing","title":"Testing","text":"<pre><code> make test\n</code></pre>"},{"location":"setup/#update-github-action-uv-version","title":"Update Github Action uv version","text":"<ul> <li>see setup-uv</li> </ul> <pre><code>- name: Install uv\n  uses: astral-sh/setup-uv@v3 -- init is uv@v2\n  with:\n    version: ${{ inputs.uv-version }}\n    enable-cache: \"true\"\n    cache-suffix: ${{ matrix.python-version }}\n</code></pre>"},{"location":"setup/#documentation-from-mkdocs","title":"Documentation from MKDocs","text":"<ul> <li> <p>navigate to Settings &gt; Actions &gt; General in your repository, and under Workflow permissions select Read and write permissions</p> </li> <li> <p>change the content in <code>./docs/index.md</code> or other files</p> </li> <li> <p>update <code>mkdocs.yml</code></p> </li> </ul> <pre><code>make docs\n</code></pre> <ul> <li> <p>after the changes are pushed to GitHub: create a new release or pre-release to trigger the <code>pages build and deployment</code> workflow.</p> </li> <li> <p>Settings &gt; Code and Automation &gt; Pages shows the documentatio URL</p> </li> <li> <p>further information on automatic documentation with MkDocs, see here.</p> </li> </ul>"},{"location":"setup/#code-coverage","title":"Code Coverage","text":"<ul> <li> <p>To enable the code coverage reports, see here.</p> </li> <li> <p>Extend action.yml:</p> </li> </ul> <pre><code>- name: Upload results to Codecov\n        uses: codecov/codecov-action@v4\n        with:\n          token: ${{ secrets.CODECOV_TOKEN }}\n</code></pre>"},{"location":"setup/#releasing-a-new-version","title":"Releasing a new version","text":"<p>Repository initiated with fpgmaas/cookiecutter-uv.</p>"}]}